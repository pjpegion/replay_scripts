export OMP_NUM_THREADS=${OMP_NUM_THREADS:-1}
[ -z "$nprocs" ] && echo "Need to set nprocs" && exit 1;
[ -z "$machine" ] && echo "Need to set machine" && exit 1;
[ -z "$PGM" ] && echo "Need to set PGM" && exit 1;
if [ ! -z $SLURM_JOB_ID ]; then
   [ -z "$mpitaskspernode" ] && echo "Need to set mpitaskspernode" && exit 1;
   # use srun
   #export OMP_PROC_BIND=spread
   #export OMP_PLACES=threads
   totcores=`expr $nprocs \* $OMP_NUM_THREADS`
   totnodes=`python -c "import math; print int(math.ceil(float(${totcores})/${corespernode}))"`
   count=`python -c "import math; print int(math.floor(float(${corespernode})/${mpitaskspernode}))"` 
   # -c: cpus per task (number of threads per mpi task)
   # -n: number of mpi tasks
   # -N: number of nodes to run on
   echo "running srun -N $totnodes -n $nprocs -c ${count} --cpu-bind=cores $PGM"
   eval srun -N $totnodes -n $nprocs -c ${count} --cpu-bind=cores $PGM
   rc=$?
elif [ "$machine" == 'wcoss' ] ; then
   [ -z "$mpitaskspernode" ] && echo "Need to set mpitaskspernode" && exit 1;
   echo "running aprun -n $nprocs -N $mpitaskspernode -d ${OMP_NUM_THREADS} --cc depth $PGM"
   eval aprun -n $nprocs -N $mpitaskspernode -d ${OMP_NUM_THREADS} --cc depth $PGM
elif [ "$machine" == 'gaea' ] ; then
   [ -z "$mpitaskspernode" ] && echo "Need to set mpitaskspernode" && exit 1;
   # use aprun
   echo "running aprun -n $nprocs -N $mpitaskspernode -d ${OMP_NUM_THREADS} --cc depth $PGM"
   eval aprun -n $nprocs -N $mpitaskspernode -d ${OMP_NUM_THREADS} --cc depth $PGM
elif [ "$machine" == 'theia' ]; then
   # HOSTFILE env var must be set
   [ -z "$HOSTFILE" ] && echo "Need to set HOSTFILE" && exit 1;
   cat $HOSTFILE
   echo "running mpirun -np $nprocs -machinefile $HOSTFILE $PGM"
   echo "OMP_NUM_THREADS = $OMP_NUM_THREADS"
   eval mpirun -np $nprocs -machinefile $HOSTFILE $PGM
   rc=$?
else
   echo "machine must be 'wcoss', 'theia', or 'gaea', got $machine"
   rc=1
fi
echo "exiting runmpi..."
exit $rc
